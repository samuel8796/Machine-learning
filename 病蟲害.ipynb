{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: KERAS_BACKEND=tensorflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%env KERAS_BACKEND = tensorflow\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,Flatten\n",
    "from keras.layers import Conv2D,MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  finished!\n",
      "1  finished!\n",
      "10  finished!\n",
      "11  finished!\n",
      "12  finished!\n",
      "13  finished!\n",
      "14  finished!\n",
      "15  finished!\n",
      "16  finished!\n",
      "17  finished!\n",
      "18  finished!\n",
      "19  finished!\n",
      "2  finished!\n",
      "20  finished!\n",
      "21  finished!\n",
      "22  finished!\n",
      "23  finished!\n",
      "24  finished!\n",
      "25  finished!\n",
      "26  finished!\n",
      "27  finished!\n",
      "28  finished!\n",
      "29  finished!\n",
      "3  finished!\n",
      "30  finished!\n",
      "31  finished!\n",
      "32  finished!\n",
      "33  finished!\n",
      "34  finished!\n",
      "35  finished!\n",
      "36  finished!\n",
      "37  finished!\n",
      "4  finished!\n",
      "5  finished!\n",
      "6  finished!\n",
      "7  finished!\n",
      "8  finished!\n",
      "9  finished!\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing.image import  img_to_array, load_img\n",
    "from PIL import Image\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "#%%\n",
    "dict_labels = {}\n",
    "size = (128,128) #由於原始資料影像大小不一，因此制定一個統一值\n",
    "nbofdata=180  #從各個資料夾中抓取特定數量的檔案\n",
    "#%% Read Traget Folders' Path\n",
    "labels=['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','21','22','23','24','25','26','27','28','29','30','31','32','33','34','35','36','37']\n",
    "base_path = r'C:\\Users\\史勝霖\\Desktop\\code\\AI summer\\38 class classification Images\\train'\n",
    "layers_of_folders=0\n",
    "folder_list=[]\n",
    "if base_path :\n",
    "    folder_layers=[]\n",
    "    files = os.scandir(base_path)\n",
    "    first_folder = []\n",
    "    first_folder_kind = []\n",
    "    for entry in files:\n",
    "        if entry.is_dir():\n",
    "            first_folder.append(entry.path)\n",
    "            first_folder_kind.append(entry.name)\n",
    "    folder_layers.append(first_folder_kind)\n",
    "    folder_list.append(first_folder)\n",
    "datanumber=nbofdata\n",
    "blob=[]\n",
    "blob_nparray=[]\n",
    "image_data=[]\n",
    "conc = 0\n",
    "labels_dict={}\n",
    "for entry1 in folder_list[layers_of_folders - 1]:\n",
    "    blob = []\n",
    "    cellname = os.path.basename(os.path.dirname(entry1))  # extract cell name\n",
    "    #print(cellname)\n",
    "    concnames = os.path.basename(entry1)  # extract concentration\n",
    "    #print(concnames)\n",
    "    if concnames in labels:\n",
    "        labels_dict[conc] = concnames\n",
    "        fnamelist = glob.glob(os.path.join(entry1, '*.jpg'))\n",
    "        for filename in fnamelist[0:datanumber]:\n",
    "            im = Image.open(filename)\n",
    "            if im is not None:\n",
    "                if im.mode=='RGB':\n",
    "                    im=im.resize(size,Image.BILINEAR)\n",
    "                    imarray = np.array(im)\n",
    "                    blob.append(imarray)\n",
    "        ind = np.reshape(np.arange(1, len(blob) + 1), (-1, 1))\n",
    "        blob_nparray = np.reshape(np.asarray(blob), (len(blob), blob[1].size))\n",
    "        blob_nparray = np.hstack((blob_nparray, ind, conc * np.ones((len(blob), 1))))\n",
    "        image_data.append(np.asarray(blob_nparray, dtype=np.float32))\n",
    "        print(concnames+'  finished!')\n",
    "        conc += 1\n",
    "#%%\n",
    "for j in range(len(labels)):\n",
    "    trytry=image_data[j][:]\n",
    "# Prepare data\n",
    "    LengthT = trytry.shape[0]\n",
    "    trytry_index = trytry[...,-2:-1]\n",
    "    trytry_label = trytry[...,-1:] #['Nega' for x in range(lengthN*4)] #Nega_data[...,-1:]\n",
    "    trytry = trytry[...,:-2]\n",
    "    \n",
    "    # Normalize image by subtracting mean image\n",
    "    trytry -= np.reshape(np.mean(trytry, axis=1), (-1,1))\n",
    "     # Reshape images\n",
    "    trytry = np.reshape(trytry, (trytry.shape[0],128,128,3))\n",
    "    \n",
    "\n",
    "    trytry = trytry.reshape(-1,128,128,3)\n",
    "    \n",
    "    \n",
    "    if j is 0:\n",
    "        train_data = trytry[:LengthT]\n",
    "      \n",
    "        train_label = trytry_label[:LengthT]\n",
    "        \n",
    "    else:\n",
    "        train_data = np.concatenate((train_data, \n",
    "                                     trytry[:LengthT]), axis=0)\n",
    "        \n",
    "        train_label = np.concatenate((train_label, \n",
    "                                     trytry_label[:LengthT]), axis=0)\n",
    "        \n",
    "train_label = keras.utils.to_categorical(train_label, num_classes=len(labels))\n",
    "\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  finished!\n",
      "1  finished!\n",
      "10  finished!\n",
      "11  finished!\n",
      "12  finished!\n",
      "13  finished!\n",
      "14  finished!\n",
      "15  finished!\n",
      "16  finished!\n",
      "17  finished!\n",
      "18  finished!\n",
      "19  finished!\n",
      "2  finished!\n",
      "20  finished!\n",
      "21  finished!\n",
      "22  finished!\n",
      "23  finished!\n",
      "24  finished!\n",
      "25  finished!\n",
      "26  finished!\n",
      "27  finished!\n",
      "28  finished!\n",
      "29  finished!\n",
      "3  finished!\n",
      "30  finished!\n",
      "31  finished!\n",
      "32  finished!\n",
      "33  finished!\n",
      "34  finished!\n",
      "35  finished!\n",
      "36  finished!\n",
      "37  finished!\n",
      "4  finished!\n",
      "5  finished!\n",
      "6  finished!\n",
      "7  finished!\n",
      "8  finished!\n",
      "9  finished!\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "dict_labels = {}\n",
    "size = (128,128) #由於原始資料影像大小不一，因此制定一個統一值\n",
    "nbofdata=38  #從各個資料夾中抓取特定數量的檔案\n",
    "#%% Read Traget Folders' Path\n",
    "labels=['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','21','22','23','24','25','26','27','28','29','30','31','32','33','34','35','36','37']\n",
    "base_path = r'C:\\Users\\史勝霖\\Desktop\\code\\AI summer\\38 class classification Images\\test'\n",
    "layers_of_folders=0\n",
    "folder_list=[]\n",
    "if base_path :\n",
    "    folder_layers=[]\n",
    "    files = os.scandir(base_path)\n",
    "    first_folder = []\n",
    "    first_folder_kind = []\n",
    "    for entry in files:\n",
    "        if entry.is_dir():\n",
    "            first_folder.append(entry.path)\n",
    "            first_folder_kind.append(entry.name)\n",
    "    folder_layers.append(first_folder_kind)\n",
    "    folder_list.append(first_folder)\n",
    "datanumber=nbofdata\n",
    "blob=[]\n",
    "blob_nparray=[]\n",
    "image_data=[]\n",
    "conc = 0\n",
    "labels_dict={}\n",
    "for entry1 in folder_list[layers_of_folders - 1]:\n",
    "    blob = []\n",
    "    cellname = os.path.basename(os.path.dirname(entry1))  # extract cell name\n",
    "    #print(cellname)\n",
    "    concnames = os.path.basename(entry1)  # extract concentration\n",
    "    #print(concnames)\n",
    "    if concnames in labels:\n",
    "        labels_dict[conc] = concnames\n",
    "        fnamelist = glob.glob(os.path.join(entry1, '*.jpg'))\n",
    "        for filename in fnamelist[0:datanumber]:\n",
    "            im = Image.open(filename)\n",
    "            if im is not None:\n",
    "                if im.mode=='RGB':\n",
    "                    im=im.resize(size,Image.BILINEAR)\n",
    "                    imarray = np.array(im)\n",
    "                    blob.append(imarray)\n",
    "        ind = np.reshape(np.arange(1, len(blob) + 1), (-1, 1))\n",
    "        blob_nparray = np.reshape(np.asarray(blob), (len(blob), blob[1].size))\n",
    "        blob_nparray = np.hstack((blob_nparray, ind, conc * np.ones((len(blob), 1))))\n",
    "        image_data.append(np.asarray(blob_nparray, dtype=np.float32))\n",
    "        print(concnames+'  finished!')\n",
    "        conc += 1\n",
    "#%%\n",
    "for j in range(len(labels)):\n",
    "    trytry=image_data[j][:]\n",
    "# Prepare data\n",
    "    LengthT = trytry.shape[0]\n",
    "    trytry_index = trytry[...,-2:-1]\n",
    "    trytry_label = trytry[...,-1:] #['Nega' for x in range(lengthN*4)] #Nega_data[...,-1:]\n",
    "    trytry = trytry[...,:-2]\n",
    "    \n",
    "    # Normalize image by subtracting mean image\n",
    "    trytry -= np.reshape(np.mean(trytry, axis=1), (-1,1))\n",
    "     # Reshape images\n",
    "    trytry = np.reshape(trytry, (trytry.shape[0],128,128,3))\n",
    "    \n",
    "\n",
    "    trytry = trytry.reshape(-1,128,128,3)\n",
    "    \n",
    "    \n",
    "    if j is 0:\n",
    "        test_data = trytry[:LengthT]\n",
    "      \n",
    "        test_label = trytry_label[:LengthT]\n",
    "        \n",
    "    else:\n",
    "        test_data = np.concatenate((test_data, \n",
    "                                     trytry[:LengthT]), axis=0)\n",
    "        \n",
    "        test_label = np.concatenate((test_label, \n",
    "                                     trytry_label[:LengthT]), axis=0)\n",
    "        \n",
    "test_label = keras.utils.to_categorical(test_label, num_classes=len(labels))\n",
    "\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Shuffle data\n",
    "import random\n",
    "temp = list(zip(train_data, train_label))\n",
    "\n",
    "random.shuffle(temp)\n",
    "\n",
    "train_data,train_label = zip(*temp)\n",
    "\n",
    "train_data=np.asarray(train_data)\n",
    "train_label=np.asarray(train_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4980 samples, validate on 1245 samples\n",
      "Epoch 1/4\n",
      "4980/4980 [==============================] - 425s 85ms/step - loss: 2.2579 - acc: 0.6607 - val_loss: 0.6718 - val_acc: 0.6337\n",
      "Epoch 2/4\n",
      "4980/4980 [==============================] - 393s 79ms/step - loss: 0.6832 - acc: 0.7108 - val_loss: 0.6491 - val_acc: 0.6123\n",
      "Epoch 3/4\n",
      "4980/4980 [==============================] - 419s 84ms/step - loss: 0.6520 - acc: 0.6463 - val_loss: 0.6400 - val_acc: 0.7919\n",
      "Epoch 4/4\n",
      "4980/4980 [==============================] - 443s 89ms/step - loss: 0.5813 - acc: 0.6902 - val_loss: 0.5440 - val_acc: 0.8756\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD,RMSprop\n",
    "# Generate model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(128,128,3),padding='same',name='block1_conv2_1'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu',padding='same',name='block1_conv2_2'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),name='block1_MaxPooling'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu',padding='same',name='block2_conv2_1'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu',padding='same',name='block2_conv2_2'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),name='block2_MaxPooling'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu',name='final_output_1'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu',name='final_output_2'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(38, activation='sigmoid',name='class_output'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=RMSprop(lr=1e-4), metrics=['accuracy'])\n",
    "#EStop = EarlyStopping(monitor='val_acc', min_delta=0, \n",
    "                      #patience=10, verbose=1, mode='auto')\n",
    "#%% Start Traning Model\n",
    "history = model.fit(train_data, train_label, batch_size=150, epochs=4,shuffle=True, validation_split=0.2)\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1163/1163 [==============================] - 36s 31ms/step\n",
      "loss: 0.5455406456801365\n",
      "acc: 0.8774946821935199\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_data,test_label)\n",
    "print('loss:',score[0])\n",
    "print('acc:',score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "open('病蟲害_model_.json','w').write(model_json)\n",
    "model.save_weights('病蟲害_weights_.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing.image import  img_to_array, load_img\n",
    "from PIL import Image\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "#%%\n",
    "dict_labels = {}\n",
    "size = (128,128) #由於原始資料影像大小不一，因此制定一個統一值\n",
    "nbofdata=177   #從各個資料夾中抓取特定數量的檔案\n",
    "#%% Read Traget Folders' Path\n",
    "labels=['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','21','22','23','24','25','26','27','28','29','30','31','32','33','34','35','36','37']\n",
    "base_path = r'C:\\Users\\史勝霖\\Desktop\\code\\AI summer\\38 class classification Images\\train'\n",
    "layers_of_folders=0\n",
    "folder_list=[]\n",
    "if base_path :\n",
    "    folder_layers=[]\n",
    "    files = os.scandir(base_path)\n",
    "    #  Get the 1st layer of folder\n",
    "    first_folder = []\n",
    "    first_folder_kind = []\n",
    "    for entry in files:\n",
    "        if entry.is_dir():\n",
    "            first_folder.append(entry.path)\n",
    "            first_folder_kind.append(entry.name)\n",
    "    folder_layers.append(first_folder_kind)\n",
    "    folder_list.append(first_folder)\n",
    "    #  Get the 2nd layer of folder\n",
    "    second_folder = []\n",
    "    if first_folder:\n",
    "        second_folder = []\n",
    "        second_folder_kind = []\n",
    "        layers_of_folders+=1\n",
    "        for fldr in first_folder:\n",
    "            files = os.scandir(fldr)\n",
    "            for entry in files:\n",
    "                if entry.is_dir():\n",
    "                    second_folder.append(entry.path)\n",
    "                    second_folder_kind.append(entry.name)\n",
    "        second_folder_kind= second_folder_kind[0:int(len(second_folder_kind)/len(first_folder_kind))]\n",
    "        folder_layers.append(second_folder_kind)\n",
    "        folder_list.append(second_folder)\n",
    "    #  Get the 3rd layer of folder\n",
    "    third_folder = []\n",
    "    if second_folder:\n",
    "        third_folder = []\n",
    "        third_folder_kind = []\n",
    "        layers_of_folders+=1\n",
    "        for fldr in second_folder:\n",
    "            files = os.scandir(fldr)\n",
    "            for entry in files:\n",
    "                if entry.is_dir():\n",
    "                    third_folder.append(entry.path)\n",
    "                    third_folder_kind.append(entry.name)\n",
    "        third_folder_kind= third_folder_kind[0:int(len(third_folder_kind)/(len(second_folder_kind)*len(first_folder_kind)))]\n",
    "        folder_list.append(third_folder)\n",
    "    #  Get the 4th layer of folder\n",
    "    forth_folder = []\n",
    "    if third_folder:\n",
    "        forth_folder = []\n",
    "        forth_folder_kind = []\n",
    "        layers_of_folders+=1\n",
    "        for fldr in third_folder:\n",
    "            files = os.scandir(fldr)\n",
    "            for entry in files:\n",
    "                if entry.is_dir():\n",
    "                    forth_folder.append(entry.path)\n",
    "                    forth_folder_kind.append(entry.name)\n",
    "        forth_folder_kind= forth_folder_kind[0:int(len(forth_folder_kind)/(len(third_folder_kind)*len(second_folder_kind)*len(first_folder_kind)))]\n",
    "        folder_list.append(forth_folder)\n",
    "     #  Get the 5th layer of folder\n",
    "    if forth_folder:\n",
    "        fifth_folder = []\n",
    "        fifth_folder_kind = []\n",
    "        layers_of_folders+=1\n",
    "        for fldr in third_folder:\n",
    "            files = os.scandir(fldr)\n",
    "            for entry in files:\n",
    "                if entry.is_dir():\n",
    "                    fifth_folder.append(entry.path)\n",
    "                    fifth_folder_kind.append(entry.name)\n",
    "        fifth_folder_kind= fifth_folder_kind[0:int(len(fifth_folder_kind)/(len(forth_folder_kind)*len(third_folder_kind)*len(second_folder_kind)*len(first_folder_kind)))]\n",
    "#%% Read Image Files (*.tif)\n",
    "datanumber=nbofdata\n",
    "blob=[]\n",
    "blob_nparray=[]\n",
    "image_data=[]\n",
    "conc = 0\n",
    "labels_dict={}\n",
    "for entry1 in folder_list[layers_of_folders - 1]:\n",
    "    blob = []\n",
    "    cellname = os.path.basename(os.path.dirname(entry1))  # extract cell name\n",
    "    # print(cellname)\n",
    "    concnames = os.path.basename(entry1)  # extract concentration\n",
    "    # print(concnames)\n",
    "    if concnames in labels:\n",
    "        labels_dict[conc] = concnames\n",
    "        fnamelist = glob.glob(os.path.join(entry1, '*.jpg'))\n",
    "        for filename in fnamelist[0:datanumber]:\n",
    "            im = Image.open(filename)\n",
    "            if im is not None:\n",
    "                if im.mode=='RGB':\n",
    "                    im=im.resize(size,Image.BILINEAR)\n",
    "                    imarray = np.array(im)\n",
    "                    blob.append(imarray)\n",
    "        ind = np.reshape(np.arange(1, len(blob) + 1), (-1, 1))\n",
    "        blob_nparray = np.reshape(np.asarray(blob), (len(blob), blob[1].size))\n",
    "        blob_nparray = np.hstack((blob_nparray, ind, conc * np.ones((len(blob), 1))))\n",
    "        image_data.append(np.asarray(blob_nparray, dtype=np.float32))\n",
    "        print(concnames+'  finished!')\n",
    "        conc += 1\n",
    "#%%\n",
    "for j in range(len(labels)):\n",
    "    trytry=image_data[j][:]\n",
    "# Prepare data\n",
    "    LengthT = trytry.shape[0]\n",
    "    trytry_index = trytry[...,-2:-1]\n",
    "    trytry_label = trytry[...,-1:] #['Nega' for x in range(lengthN*4)] #Nega_data[...,-1:]\n",
    "    trytry = trytry[...,:-2]\n",
    "    \n",
    "    # Normalize image by subtracting mean image\n",
    "    trytry -= np.reshape(np.mean(trytry, axis=1), (-1,1))\n",
    "    # Reshape images\n",
    "    trytry = np.reshape(trytry, (trytry.shape[0],128,128,3))\n",
    "    \n",
    "\n",
    "    trytry = trytry.reshape(-1,128,128,3)\n",
    "    np.random.shuffle(trytry)\n",
    "    trytry_train_upto = round(trytry.shape[0] * 8 / 10)\n",
    "    trytry_test_upto = trytry.shape[0]\n",
    "    if j is 0:\n",
    "        train_data = trytry[:trytry_train_upto]\n",
    "        test_data = trytry[trytry_train_upto:trytry_test_upto]\n",
    "        train_label = trytry_label[:trytry_train_upto]\n",
    "        test_label = trytry_label[trytry_train_upto:trytry_test_upto]\n",
    "        \n",
    "    else:\n",
    "        train_data = np.concatenate((train_data, \n",
    "                                     trytry[:trytry_train_upto]), axis=0)\n",
    "        \n",
    "        test_data = np.concatenate((test_data, \n",
    "                                    trytry[trytry_train_upto:trytry_test_upto]), axis=0)\n",
    "        \n",
    "        train_label = np.concatenate((train_label, \n",
    "                                     trytry_label[:trytry_train_upto]), axis=0)\n",
    "        \n",
    "        \n",
    "        test_label = np.concatenate((test_label, \n",
    "                                    trytry_label[trytry_train_upto:trytry_test_upto]), axis=0)\n",
    "        \n",
    "test_label = keras.utils.to_categorical(test_label, num_classes=len(labels))\n",
    "train_label = keras.utils.to_categorical(train_label, num_classes=len(labels))\n",
    "\n",
    "\n",
    "print(\"done\")\n",
    "\n",
    "#%% Shuffle data\n",
    "import random\n",
    "temp = list(zip(train_data, train_label))\n",
    "\n",
    "random.shuffle(temp)\n",
    "\n",
    "train_data,train_label = zip(*temp)\n",
    "\n",
    "train_data=np.asarray(train_data)\n",
    "train_label=np.asarray(train_label)\n",
    "\n",
    "#%% VGG 16 only for classification\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD,RMSprop\n",
    "# Generate model\n",
    "model = Sequential()\n",
    "# input: 190x190 images with 3 channels -> (190, 190, 3) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(128,128,3),padding='same',name='block1_conv2_1'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu',padding='same',name='block1_conv2_2'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),name='block1_MaxPooling'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu',padding='same',name='block2_conv2_1'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu',padding='same',name='block2_conv2_2'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),name='block2_MaxPooling'))\n",
    "model.add(Dropout(0.25))\n",
    "#\n",
    "#model.add(Conv2D(256, (3, 3), activation='relu',padding='same',name='block3_conv2_1'))\n",
    "#model.add(Conv2D(256, (3, 3), activation='relu',padding='same',name='block3_conv2_2'))\n",
    "#model.add(Conv2D(256, (3, 3), activation='relu',padding='same',name='block3_conv2_3'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2),name='block3_MaxPooling'))\n",
    "#model.add(Dropout(0.25))\n",
    "#\n",
    "#model.add(Conv2D(512, (3, 3), activation='relu',padding='same',name='block4_conv2_1'))\n",
    "#model.add(Conv2D(512, (3, 3), activation='relu',padding='same',name='block4_conv2_2'))\n",
    "#model.add(Conv2D(512, (3, 3), activation='relu',padding='same',name='block4_conv2_3'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2),name='block4_MaxPooling'))\n",
    "#model.add(Dropout(0.25))\n",
    "#\n",
    "#model.add(Conv2D(512, (3, 3), activation='relu',padding='same',name='block5_conv2_1'))\n",
    "#model.add(Conv2D(512, (3, 3), activation='relu',padding='same',name='block5_conv2_2'))\n",
    "#model.add(Conv2D(512, (3, 3), activation='relu',padding='same',name='block5_conv2_3'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2),name='block5_MaxPooling'))\n",
    "#model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu',name='final_output_1'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu',name='final_output_2'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(38, activation='sigmoid',name='class_output'))\n",
    "optimizer = RMSprop(lr=1e-4)\n",
    "objective = 'binary_crossentropy'\n",
    "model.compile(loss=objective, optimizer=optimizer, metrics=['accuracy'])\n",
    "EStop = EarlyStopping(monitor='val_acc', min_delta=0, \n",
    "                      patience=10, verbose=1, mode='auto')\n",
    "#%% Start Traning Model\n",
    "history = model.fit(train_data, train_label, batch_size=64, epochs=10,shuffle=True, validation_split=0.2,callbacks=[EStop])\n",
    "#%%\n",
    "model.save('病蟲害_model.h5') \n",
    "predictions=model.predict(test_data)\n",
    "#%%\n",
    "from keras.preprocessing.image import array_to_img\n",
    "from ipywidgets import interact_manual\n",
    "\n",
    "predict = model.predict_classes(test_data)\n",
    "\n",
    "def test(number):\n",
    "    plt.imshow(test_data[number].reshape(28,28),cmap = 'Greys')\n",
    "    print(\"the network predict as :\",predict[number])\n",
    "\n",
    "interact_manual(test, number = (0,786));\n",
    "\n",
    "plt.imshow(array_to_img(test_data[i]))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
